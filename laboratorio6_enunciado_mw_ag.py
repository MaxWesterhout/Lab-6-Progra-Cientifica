# -*- coding: utf-8 -*-
"""Laboratorio6_Enunciado_MW_AG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XS-IwtiDHExTJxw2C3s2DnsUxmFbvram

<h1><center>Laboratorio 6: El Pandas no Muerde (act III) üêº</center></h1>

<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos</strong></center>

**ACT III????**

![resultados](https://static.wikia.nocookie.net/f6a10ad9-4e02-46c8-8874-9e9eea9451d9/scale-to-width/755 "res")

### Cuerpo Docente:

- Profesores: Ignacio Meza, Gabriel Iturra
- Auxiliar: Sebasti√°n Tinoco
- Ayudantes: Arturo Lazcano, Angelo Mu√±oz

### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados

- Nombre de alumno 1: Maximiliano Westerhout
- Nombre de alumno 2: Alvaro Gallardo

### **Link de repositorio de GitHub:** `https://github.com/MaxWesterhout/Lab-6-Progra-Cientifica.git`

## Reglas:

- **Grupos de 2 personas**
- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.
- Prohibidas las copias.
- Pueden usar cualquer matrial del curso que estimen conveniente.

### Objetivos principales del laboratorio

- Aplicar los paradigmas y buenas pr√°cticas de programaci√≥n vistas hasta este momento.
- Comprender y aprovechar las ventajas que nos ofrece la liberia `numpy` con respecto a trabajar en Python 'puro'.
- Visualizar aplicaciones de filtros de im√°genes sin el uso de librer√≠as.
- Verificar que el uso indiscriminado de `for` puede afectar en la eficiencia en al procesar datos masivos.


El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka "for", "while"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*).

## Descripci√≥n del laboratorio.

### Importamos librerias utiles üò∏
"""

# Libreria Core del lab.
import numpy as np
import pandas as pd
import datetime
from scipy import stats

from IPython.display import display, Markdown, Latex

#Libreria para plotear
import matplotlib.pyplot as plt
import missingno as msno
import plotly.express as px

# Si usted est√° utilizando Colabolatory le puede ser √∫til este c√≥digo para cargar los archivos.
try:
    from google.colab import drive
    drive.mount("/content/drive")
    path = 'Direcci√≥n donde tiene los archivos en el Drive'
except:
    print('Ignorando conexi√≥n drive-colab')

"""# Segmentaci√≥n de Clientes en Tienda de Retail üõçÔ∏è

<p align="center">
  <img width=300 src="https://s1.eestatic.com/2018/04/14/social/la_jungla_-_social_299733421_73842361_854x640.jpg">
</p>

## 1.1 Cargar Dataset

Mr. Lepin Mora quien es el gerente de una cotizada tienda de retail en Europa, les solicita si pueden analizar los datos de algunas de sus tiendas y si es posible extraer los diferenciar los tipos de clientes que posee el retail.

Para esto, el √°rea de ventas les entrega el archivo `online_retail_II.xlsx` con el que se les pide que cargue y visualicen algunas de las filas que componen el Dataset.

Realice una primera visualizaci√≥n de los datos y se√±ale los atributos que componen el dataset. Se√±ale las columnas que conforman el dataset, el tipo de variable presente en cada columna y comente que representa cada una de estas.

**Respuesta:**
"""

df_retail = pd.read_pickle('/content/online_retail_II.pickle')
df_retail = df_retail.astype(
    {
        "Invoice": "category",
        "StockCode": "category",
        "Description": str,
        "Customer ID": "category",
    }
)
df_retail.head()

df_retail[df_retail['Description'] == '15CM CHRISTMAS GLASS BALL 20 LIGHTS']

df_retail[df_retail['Quantity'] == 19152]

df_retail.info()

df_retail.describe()

"""El DataFrame contiene aproximadamente 525,461 registros de compras y est√° compuesto por 8 atributos que se describen a continuaci√≥n:

Invoice

Contiene diferentes n√∫meros que representan el n√∫mero de factura. Esta variable es de tipo categ√≥rica.   

StockCode

Contiene diferentes n√∫meros que corresponden al c√≥digo del producto. Esta variable es de tipo categ√≥rica.

Description

Contiene texto que describe el producto. Esta variable es de tipo objeto y representa una cadena de caracteres, es decir, un string.

Quantity

Contiene diferentes n√∫meros que representan la cantidad del producto comprada. Esta variable es de tipo num√©rica y se representa como un entero (int).      

InvoiceDate

Contiene fechas que indican la fecha en que se emiti√≥ la factura del producto. Esta variable es de tipo num√©rica y se representa como una fecha y hora (datetime).

Price

Contiene n√∫meros que representan el precio del producto. Esta variable es de tipo num√©rica y se representa como un n√∫mero decimal(float).         

Customer ID

Contiene diferentes n√∫meros que representan el ID del cliente que realiz√≥ la compra. Esta variable es de tipo categ√≥rica.      

Country

Contiene texto que indica el pa√≠s donde se realiz√≥ la compra del producto. Esta variable es de tipo objeto y representa una cadena de caracteres (string)

## 1.2 An√°lisis Explotatorio de los Datos [0.5 puntos]

En base a la primera visualizaci√≥n del dataset, *Don Mora* le solicita que realicen un an√°lisis exploratorio de los datos, para esto les deber√°n realizar un an√°lisis univariado y multivariado. De la revisi√≥n, ustedes deben explicar potenciales anomal√≠as visualizadas y se√±alar si existe la necesidad de realizar una limpieza de datos.

Explique a que nos referimos con an√°lisis univariable, multivariable y de datos faltantes. ¬øQu√© beneficios nos otorga estudiar estos datos?. Sea conciso con su respuesta y no escriba mas de 5 l√≠neas para su respuesta.

**Respuesta a la Pregunta:**

El an√°lisis univariable se centra en el analisis de una variable a la vez para comprender su distribuci√≥n, estadisticas descriptivas y posibles anomal√≠as.
Por otro lado, el an√°lisis multivariable analiza relaciones entre variables, logrando revelar patrones, correlaciones y dependencias entre las variables. Finalmente la revisi√≥n de datos faltantes es la detecci√≥n de valores que no estan presentes en el conjunto de datos.
Estudiar estos datos nos entrega el entendimiento completo de toda la informaci√≥n, mejorando la detecci√≥n de problemas y la toma de decisiones con el fin de mejorar la calidad de datos

### 1.2.1 An√°lisis Univariado [2 Puntos]

A continuaci√≥n, se le presentan dos funciones para analizar los datos que componen un dataframe. La primera de estas es la funci√≥n ``profile_serie()`` la cual recibe una serie y le entrega un an√°lisis detallado de los datos que conforman dicha serie.

Ejecute la funcion ``profile_serie()`` sobre cada serie para realizar un an√°lisis univariado de estas. A continuaci√≥n, comente acerca de el comportamiento de cada variable seg√∫n las estad√≠sticas descriptivas y los gr√°ficos generados.
"""

from pandas.api.types import is_numeric_dtype
from pandas.core.dtypes.common import is_datetime_or_timedelta_dtype


def profile_serie(serie_in, n_samples=1000, random_state=42):
    serie = serie_in.copy()

    profile = pd.Series(dtype='object')
    profile["Type"] = serie.dtype
    profile = pd.concat([profile, serie.describe(datetime_is_numeric=True)])

    # profile = pd.Series([])

    if is_numeric_dtype(serie):
        profile["Negative"] = (serie < 0).sum()
        profile["Negative (%)"] = (
            str(round((serie < 0).sum() / len(serie) * 100, 2)) + " %"
        )
        profile["Zeros"] = (serie == 0).sum()
        profile["Zeros (%)"] = (
            str(round((serie == 0).sum() / len(serie) * 100, 2)) + " %"
        )
        profile["Kurt"] = serie.kurt()
        profile["Skew"] = serie.skew()

    profile[" "] = " "  # espacio

    profile["Missing cells"] = serie.isnull().sum()
    profile["Missing cells (%)"] = (
        str(round(serie.isnull().sum() / len(serie) * 100, 2)) + " %"
    )
    profile["Duplicate rows"] = serie.duplicated(False).sum()
    profile["Duplicate rows (%)"] = (
        str(round(serie.duplicated(False).sum() / len(serie) * 100, 2)) + " %"
    )
    profile["Total size in memory"] = str(serie.memory_usage(index=True)) + " bytes"

    # profile = pd.concat([profile, description])

    profile = profile.rename(
        index={
            "count": "Number of observations",
            "mean": "Mean",
            "std": "Std",
            "min": "Min",
            "max": "Max",
            "unique": "Unique",
            "top": "Top",
            "freq": "Freq",
        }
    )
    no_outliers_fig = None

    if is_numeric_dtype(serie):

        sampled_serie = serie.sample(n_samples, random_state=random_state)
        fig = px.histogram(
            sampled_serie, marginal="box", title=f"{serie.name} - With Outliers"
        )

        no_outliers = sampled_serie.loc[(np.abs(stats.zscore(sampled_serie)) < 3)]
        # zscore = https://es.wikipedia.org/wiki/Unidad_tipificada

        no_outliers_fig = px.histogram(
            no_outliers, marginal="box", title=f"{serie.name} - Without Outliers"
        )

    elif is_datetime_or_timedelta_dtype(serie):
        sampled_serie = serie.sample(n_samples, random_state=random_state)
        fig = px.histogram(sampled_serie, marginal="box", title=f"{serie.name}")

    else:
            count = (
                serie.value_counts()[0:100]
                .reset_index()
                .rename(columns = {serie.name: 'Count'})
            )
            fig = px.bar(
                x=count['index'].astype(str),
                y=count["Count"],
                title=f"100 Most common categories of {serie.name}",
            )

    display(Markdown(f'## {serie.name} Profile'))
    display(profile)
    fig.show()

    if no_outliers_fig:
        no_outliers_fig.show()

    # return fig, profile

profile_serie(df_retail['Invoice'])

"""An√°lisis de la primera serie...

La primera serie corresponde a la variable [Invoice] la cual se puede traducir como factura. Los valores de esta variable corresponden a numeros que cumplen una especie de codigo, existen 28816 codigos unicos de los cuales el que mas se repite es el 537434 con un total de 675 repeticiones. Dicha informacion puede arrojar cual es el producto mas comprado o servicio entregado dado que a priori no se sabe si puede existir una misma factura para distintos productos.
El resto de las frecuencias va disminuyendo en peque√±a medida para el resto de las facturas, siendo muy similares los numeros entre ellos.
"""

profile_serie(df_retail['StockCode'])

"""An√°lisis de la segunda serie...

La segunda serie corresponde a la variable [StockCode], similar a la variable anterior proporciona informacion de las ventas de los productos teniendo codigos para cada Stock de productos, siendo el codigo mas comun  85123A con una frecuencia de 3516. La diferencia entre el codigo mas comun y el segundo es abrupta, siendo una diferencia de 1295, el resto de los codigos desciende de forma menos abrupta teniendo valores similares entre ellos.
"""

profile_serie(df_retail['Description'])

"""An√°lisis de la tercera serie...

La tercera serie corresponde a la variable [Description], esta variable corresponde a la descripcion del objeto comprado, en donde el valor mas frecuente corresponde a WHITE HANGING HEART T-LIGHT HOLDER, que corresponde a una especie de colgador de luces siendo el valor de su frecuencia de 3549. Para el resto de los objetos la cantidad es bastante similar y decrece poco a poco.
"""

profile_serie(df_retail['Quantity'])

"""An√°lisis de la cuarta serie...

La cuarta serie corresponde a la variable [Quantity], la cual corresponde a la cantidad del producto. El primer grafico considera los outliers de la variable, en donde existe una gran diferencia dado que el maximo de esta variable corresponde a 19152 y el minimo a -9600, siendo el promedio de estos datos presente en 10.33. Debido a esto, se pierde informacion en el grafico dado que no se puede observar bien la variacion entre los datos presentes entre 0 a 13 dado el extenso rango de los datos, esto tambien afecta el valor de la desviacion estandar (std) la cual tiene un valor de 107.42 que es demasiado en comparacion al histograma evidenciado.

Al observar el grafico sin outlayers, se logra evidenciar mas los valores presentes entre -20 a 100, en donde se concentra la mayor cantidad de datos entre 0 y 13 (lo cual explica el valor del promedio), siendo los valores mas frecuentes entre 0 y 1.

Cabe destacar que en esta variable existe una gran cantidad de datos negativos lo cual a priori no tiene sentido dentro del contexto del problema dado que no existen cantidades negativas (a menos de que se deba o se de un sentido).
"""

profile_serie(df_retail['InvoiceDate'])

"""An√°lisis de la quinta serie...

La quinta serie corresponde a la variable ['InvoiceDate'], la cual se traduce como fecha de la factura. Esta variable es de tipo datatime64 y permite generar un histograma que contemple los distintos rangos de meses. Dicho grafico arroja que el dia con mas facturas corresponde al 7 de Noviembre del 2010 con un total de 69 facturas generadas. No existen ni missing values ni null.


"""

profile_serie(df_retail['Price'])

"""An√°lisis de la sexta serie...

La quinta serie corresponde a la variable [Price], que contempla los precios de los articulos de retail comprados. Al analizar las metricas notamos que la media de los precios se encuentra en 4.688 teniendo una desviacion estandar de 146.12, esto significa que en general los precios se maneja en valores cercanos a 4.6 pero a la vez tienden a dispersarse en gran medida. Ademas de esto, podemos notar que existen valores negativos dado que el minimo corresponde a -53594.36 pero solo existiendo 3 precios negativos lo cual podria considerarse como un fallo dado el numero de datos correspondientes al dataset (son despreciables). Respecto al maximo nos encontramos con que dicho valor corresponde a 25111.09 mucho mas que la media, lo cual nos da un indicativo de por que la desviacion estandar es tan grande similar al considerar el minimo antes expuesto.

Respecto a los histogramas, el primero que considera los outlayers apunta que existe una gran cantidad (813 especificamente) de muestras entre los valores -5 a 4.99 lo cual tiene sentido dado los boxplots y numero de media calculado, pero dicho histograma pierde informacion dado que existen outlayers en una escala mucho mas grande. Al analizar el histograma sin outlayers, podemos notar que la mayor cantidad de datos se centra entre 0.5 y 1.49 habiendo 270 datos en dicha seccion, disminuyendo progresivamente en medida de que aumenta el precio.

Por lo tanto, se puede inferir que es mas comun que hayan compras para precios peque√±os que para valores altos, y que en general los productos con valores entre 0.5 y 1.49 son los mas vendidos.
"""

profile_serie(df_retail['Customer ID'])

"""An√°lisis de la septima serie...

La septima serie corresponde a la variable ['Customer ID'], dichos valores corresponden a un codigo que repesenta el ID del cliente. Dichos codigos se repiten en gran medida, siendo el mas comun 14911 con 5710 repeticiones. El resto de los valores desciende de forma abrupta manteniendose generalmente en valores cercanos al 800.

Cabe destacar que para esta variable, existen 107927 missing cells correspondientes al 20.54% de los datos, asociar por que existe tantos datos perdidos corresponde a una descion que tomar referido al contexto del problema.
"""

profile_serie(df_retail['Country'])

"""An√°lisis de la octava serie....

La octava serie se relaciona con la variable [Country], la cual representa los pa√≠ses donde se efect√∫an las distintas compras. En esta categor√≠a, se detectan aproximadamente 40 naciones distintas, destacando sobre todas ellas el United Kingdom, con un impresionante total de 485,852k registros. Este pa√≠s, por s√≠ solo, engloba pr√°cticamente la totalidad de las compras registradas en el dataset, ya que el siguiente en la lista es EIRE, con tan solo 9,670 registros.

### 1.2.2 An√°lisis Multivariado y Datos Faltantes [1 ptos]

En segundo lugar encontrar√° la funci√≥n ``profile_df()`` que recibe un dataframe como entrada y realiza un an√°lisis bivariado de todas las variables num√©ricas que conforman el dataframe, un analisis de la correlaci√≥n de Pearson entre las variables numericas del dataframe y la matriz de datos faltantes. Ejecute la funci√≥n `profile_df` y comente sus resultados.
"""

def profile_df(dataframe_in):
    df = dataframe_in.copy()

    list_type = []
    for col in list(df.columns):
        if is_numeric_dtype(df[col]) or \
        pd.core.dtypes.common.is_datetime_or_timedelta_dtype(df[col]):
            list_type.append(col)


    display(Markdown('## Bivariant Analysis:'))
    for i in range(len(list_type)):
        for j in range(i+1, len(list_type)):
            plt.scatter(df[list_type[i]], df[list_type[j]])
            plt.xlabel(list_type[i])
            plt.ylabel(list_type[j])
            plt.title(f"{list_type[i]} v/s {list_type[j]}")
            plt.show()

    display(Markdown('## Correlation:'))
    fig_corr = px.imshow(df[list_type].corr())
    fig_corr.show()

    display(Markdown('## Missing Matrix:'))
    fig, ax = plt.subplots(figsize=[15, 10])
    msno.matrix(df, ax=ax, sparkline=False)

profile_df(df_retail)

"""Referido a los gr√°ficos anteriores se pueden identificar irregularidades en los datos, espec√≠ficamente en los atributos Quantity y Prices (figura 2), donde se registran valores negativos. Esto plantea un problema ya que en el contexto de productos, no deber√≠a haber cantidades negativas ni precios.

En base a esto, se puede decir que existen posibles errores en la recopilaci√≥n de datos. Por lo tanto, se debe realizar una limpieza de datos para eliminar los valores negativos tanto en la variable Price como en Quantity.
Por otro lado, al analizar la gr√°fica de la matriz de datos faltantes (figura 5), se muestra que el atributo Customer ID presenta una gran cantidad de valores faltantes. En este caso se podr√≠a considerar el remplazo del valor faltante por un NONE CLIENT que se considerara como dichos clientes que compran como invitado‚Äù o simplemente no dan sus datos.

Finalmente, se puede establecer que no existe una correlaci√≥n aparente entre las variables num√©ricas en el conjunto de datos, es decir, entre Quantity y Price dada la matriz de correlacion de la figura 4. Esta conclusion tambien aplica para los graficos 1 a 3, en donde no existe ninguna relacion entre las variables graficadas.

### 1.2.3 Limpieza de Datos [1 pto]

Como pudo ver en las secciones anteriores, los datos presentan valores erroneos, es por esto que se le solicita que genere una funci√≥n que permita limpiar el dataset. Realice esta funci√≥n en base observaciones propias y considere como imposible tener cantidades negativas en las ventas.

Una vez realizada la funci√≥n, realice nuevamente el an√°lisis exploratorio y comente las principales diferencias.

**Respuesta:**
"""

import numpy as np

def limpiar_dataset(df):
    #Se copia el dataset
    df1 = df.copy()
    #Se toman los valores positivos
    df1 = df[(df['Quantity'] > 0) & (df['Price'] > 0)]
    #Se a√±ade la categoria NONE CLIENT
    nueva_categoria = 'NONE CLIENT'
    #Se reemplazan los missing values
    df1['Customer ID'] = df1['Customer ID'].cat.add_categories(nueva_categoria)
    df1['Customer ID'].fillna(nueva_categoria, inplace=True)
    return df1

"""La limpieza generada en cuestio primero crea una copia del dataset y luego procesde a seleccionar los valores que poseen Quantity y Price positivos, se tomo dicha decision dado que no tienen sentidos cantidades ni precios negativos bajo el contexto de una tienda. Finalmente, se decidio a√±adir el dato NONE CLIENT, referido a los clientes que no poseen un ID, se asocian dichas ventas a clientes que no se registraron al pagar o que simplemente no decidieron dar sus datos para generar la venta."""

df_retail

df_clean = limpiar_dataset(df_retail)
df_clean

df_clean.describe()

profile_serie(df_clean['Invoice'])

"""An√°lisis de la primera serie...

La primera serie corresponde a la variable [Invoice] la cual se puede traducir como factura. Pare este caso, el grafico se mantiene completamente igual sin haber ninguna variacion aparente.

"""

profile_serie(df_clean['StockCode'])

"""An√°lisis de la segunda serie...

La segunda serie corresponde a la variable [StockCode], teniendo para este caso ninguna variacion visual para el histograma. Referido a la frecuecia maxima del valor TOP, se tiene que vario reduciendose de 3516 a 3421 esto tiene sentido dado que al limpiar el dataset se elimanoren valores lo cual afecta la frecuencia de los distintos codigos.
"""

profile_serie(df_clean['Description'])

"""Para la categoria de ['Description'], se mantiene el mismo histograma sin haber ninguna diferencia aperente. Al entrar en detalle se nota que hubo una diferencia respecto a la frecuencia del valor TOP, reduciendose en una peque√±a proporcion (nuevamente por los valores negativos)."""

profile_serie(df_clean['Quantity'])

"""Referido a la variable ['Quantity'], cambio el rango debido a que ya no existen valores negativos. Aun asi, el grafico con outlayers no permite observar de manera tan clara la distribucion de los datos, siendo mejor observar el segundo. Cabe destacar que aumento el valor del promedio dado la inexistencia de los valores negativos pero se mantuvo la misma distribucion."""

profile_serie(df_clean['InvoiceDate'])

"""Para el caso de la variable ['InvoiceDate'], se tiene que vario la distribucion pero se mantuvo que Noviembre 7 es el valor con mayor frecuencia. Aun asi, el pick que mantenia dicha fecha antes era mucho mas pronunciado por lo cual se puede asumir que gran parte de los valores de dicha fecha eran valores negativos."""

profile_serie(df_clean['Price'])

"""Para la variable ['Price'], ocurre algo similar que para el caso de Quantity, en donde al no existir valores negativos se aprecian de mejor forma los histogramas pero de todos modos los outlayers siguen siendo un problema (en terminos de visualizacion). Cabe destacar que se redujo el valor del promedio, dado la falta de numeros negativos, siendo esta variacion de 0.4 pero se mantuvo la misma distribucion.

Este ultmo resultado apunta que el promedio se estaba viendo afectado por los oulayers negativos, lo cual tiene sentido debido a la formula de esta metrica, aun asi, los oulayers positivos siguen afectando estos valores dado que estos no se centran en 4.2 sino mas bien en 1.5 lo cual se logra apreciar en el hsitograma. Esto es normal al considerar que existen oulayers con valores de 25111.
"""

profile_serie(df_clean['Customer ID'])

"""Referido a la variable ['Customer ID'], se tiene que al reemplazar los missing values con NONE CLIENT se genera que dicho codigo sea el mas frecuente con una cantidad de 103.902k, esto tiene sentido en el contexto del problema dado que muchos clientes no les gusta dar su informacion si no es relevante. Cabe destacar que decidimos utilizar dichos datos y no directamente sacarlos, debido a que es relevante para la tienda saber que existe un gran porcenaje de clientes que no poseen ID, quizas deban generar mas incentivos para que se registren."""

profile_serie(df_clean['Country'])

"""Luego de aplicar el proceso de limpieza de datos, se mantiene la calidad del conjunto de datos para el atributo [‚ÄòCountry‚Äô]. Por lo tanto no tiene alguna variaci√≥n visual en √©l."""

profile_df(df_clean)

"""Al quitar los datos negativos se aprecian patrones mas claros teniendo lo siguiente:

Referido a los gr√°ficos se puede ver una relaci√≥n entre las fechas y las cantidades vendidas, lo cual puede explicarse por la influencia de la estacionalidad en la demanda, a priori no existe una relacion directa pero para ciertas fechas hay mas variaciones de precios. En ciertos momentos del a√±o, algunos productos son m√°s susceptibles a compras debido a eventos estacionales que motivan la compra, ejemplo el Ciberday.

En relaci√≥n con el gr√°fico que compara la cantidad con el precio, no se observa una tendencia directa dado que existen productos con mucha cantidad a muy bajos precios y muchos productos a baja cantidad a precios muy caros. Este resultado tambien se relaciona con el valor de la matriz de correlacion de mas adelante.
En cuanto a las fechas de las facturas y los precios, se aprecia una estabilidad general a lo largo del a√±o, con algunas variaciones menores. No obstante, cerca del final del a√±o, en los meses de abril y mayo, as√≠ como en octubre y noviembre, se registra un leve aumento en los precios de los productos, aunque este incremento no resulta significativo.

Referido a la matriz de correlacion no existe relacion directa entre las variables , por ulitmo, para la matriz de valores faltanates no existe ninguno debido a la limpieza implementada.

### 1.2.4 Obtenci√≥n de TOPs [0.75 ptos]

Sin considerar los comentarios realizados en la secci√≥n 1.2 , *Don Mora* les pide obtener el **Top de 30 productos que generan m√°s ganancias** para la tienda de retail. Deben considerar todo el registro temporal presente en el dataset y entregar la informaci√≥n en un gr√°fico de barras de los ingresos/cantidades v/s el nombre de los productos (Utilice `plotly`). ¬øLos art√≠culos m√°s vendidos son los mismos que generan m√°s ganancias?, Comente los resultados obtenidos.

**Resultados:**
"""

df_clean['Ganancias'] = df_clean['Price'] * df_clean['Quantity'] # Calculamos las ganancias para cada producto (Precio * Cantidad)
top_products = df_clean.groupby('Description')[['Ganancias', 'Quantity']].sum().reset_index()# Agrupamos por el nombre del producto y sumamos las ganancias y la cantidad
top_30_products = top_products.sort_values(by='Ganancias', ascending=False).head(30)# Ordenamos los productos por ganancias de manera descendente y tomamos los primeros 30

fig = px.bar(top_30_products, x='Description', y=['Ganancias', 'Quantity'],
             title='Top 30 Productos con Mayor Ganancia y Cantidad Vendida',
             labels={'Description': 'Nombre del Producto', 'value': 'Valor'},
             height=800)
fig.update_layout(barmode='group')
fig.show()

"""Al analizar el gr√°fico, se destaca que el producto "Manual" es el que genera m√°s ganancias, calculado como el producto del precio por la cantidad vendida, con un total de 2,629,921K de ingresos. No obstante, se puede notar que este producto no es el m√°s vendido, ya que registra apenas 2,831 unidades vendidas. Por otro lado, el producto m√°s vendido, "WHITE HANGING HEART T-LIGHT HOLDER," con un total de 58,792K unidades vendidas, ocupa el tercer lugar en ingresos, con un total de 1,606,308K en ingresos.

En el caso de los productos que est√°n en el top 30 de ingresos, se observa que no existe una relaci√≥n directa entre la cantidad de productos vendidos y los ingresos generados. Esto se debe a que el precio de cada producto influye en los ingresos totales. Por lo tanto, es un factor que se debe tener en cuenta al analizar esta m√©trica

### 1.2.5 Visualizaci√≥n del registro temporal [0,75 ptos]

El due√±o del retail en su af√°n por saber m√°s sobre los datos de su firma les solicita que grafiquen las ventas respecto al tiempo. Con esto les aclara que durante el d√≠a tienen muchas variaciones en sus ventas, por lo que les recomienda que consideren el registro temporal como `a√±o-mes-d√≠a`. ¬øEs posible observar datos extra√±os?, Comente lo que observa del gr√°fico.
"""

def plot_ventas(dataframe):
  dataframe = dataframe.copy()
  dataframe['InvoiceDate'] = pd.to_datetime(dataframe['InvoiceDate'])
  ventas_por_fecha = dataframe.groupby(dataframe['InvoiceDate'].dt.strftime('%Y-%m-%d'))[['Ganancias', 'Quantity']].sum().reset_index()
  fig = px.line(ventas_por_fecha, x='InvoiceDate', y=['Ganancias', 'Quantity'], title='Ventas y ganancias a lo largo del tiempo',
                labels={'InvoiceDate': 'Fecha',  'value': 'Valor'})
  fig.update_xaxes(type='category')
  fig.show()

plot_ventas(df_clean)

"""En el siguiente grafico se puede apreciar que existen varios picks de ventas en ciertas fechas del a√±o, esto nos confirma la relaci√≥n establecida gracias al analisis exploratorio anterior.

Podemos tomar como ejemplo que pueden existir productos que se vendan de mayor manera en cierta fecha pero que resulten tener un precio bajo y por ello se venden m√°s, esto nos genera picks de una mayor venta pero no necesariamente una mayor ganancia. Esto se puede considerar como una anomalia pero tiene sentido debido al contexto del problema.

# Conclusi√≥n
Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.

![Gracias Totales!](https://i.pinimg.com/originals/65/ae/27/65ae270df87c3c4adcea997e48f60852.gif "bruno")

<br>
<center>
<img src="https://i.kym-cdn.com/photos/images/original/001/194/195/b18.png" width=100 height=50 />
</center>
<br>

**Esto es PANDAS REQUIEEEM:**

![resultados](https://media.tenor.com/274j_RLtt6UAAAAC/giorno-gold-experience.gif "res")

<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""